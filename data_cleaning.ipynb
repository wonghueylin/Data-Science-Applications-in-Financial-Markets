{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "\n",
    "    #======================================== SETTINGS ====================================================\n",
    "    base_ori_folder: str = r'c:\\Users\\Desktop\\Ori Data' \n",
    "    base_dest_folder: str = r'c:\\Users\\Desktop\\Clean Data'\n",
    "\n",
    "    # Date checking interval, day='D', hour='h', min='ME'\n",
    "    frequency: str = 'ME'\n",
    "    #======================================================================================================\n",
    "\n",
    "    time_column: str = 'start_time' # for date checking\n",
    "    required_columns: List[str] = None\n",
    "    columns_to_drop: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.required_columns = ['start_time', 'end_time', 'close', 'volume']\n",
    "        self.columns_to_drop = ['open', 'high', 'low', 'blockheight', 't', 'date', 'datetime']\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_log_file_path(dest_folder: str, log_type: str, folder_name: str) -> str:\n",
    "        \"\"\"Generate log file path with folder name as prefix\"\"\"\n",
    "        return os.path.join(dest_folder, 'log', f'{folder_name}_{log_type}_log.txt')\n",
    "\n",
    "    def write_log(self, log_path: str, messages: List[str]) -> None:\n",
    "        \"\"\"Write messages to log file if there are any\"\"\"\n",
    "        if messages:\n",
    "            os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "            with open(log_path, 'a') as f:\n",
    "                f.write('\\n'.join(messages) + '\\n')\n",
    "\n",
    "    def check_dates(self, df: pd.DataFrame) -> Tuple[bool, Optional[pd.DatetimeIndex]]:\n",
    "        \"\"\"Check for missing dates in the dataframe\"\"\"\n",
    "        try:\n",
    "            # df[self.config.time_column] = pd.to_datetime(df[self.config.time_column])\n",
    "            df[self.config.time_column] = pd.to_datetime(df[self.config.time_column], dayfirst=True)\n",
    "            if df[self.config.time_column].isna().all():\n",
    "                return False, \"Empty or invalid date column\"\n",
    "                \n",
    "            date_range = pd.date_range(\n",
    "                start=df[self.config.time_column].min(),\n",
    "                end=df[self.config.time_column].max(),\n",
    "                freq=self.config.frequency\n",
    "            )\n",
    "            missing_dates = date_range.difference(df[self.config.time_column])\n",
    "            return missing_dates.empty, missing_dates\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in check_dates: {str(e)}\")\n",
    "            return False, None\n",
    "        \n",
    "    def separate_and_save_factors(self, df: pd.DataFrame, file_path: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Separate factors and return dictionary of factor dataframes\"\"\"\n",
    "        factor_dfs = {}\n",
    "        factor_columns = [col for col in df.columns if col not in self.config.required_columns]\n",
    "        \n",
    "        for factor in factor_columns:\n",
    "            columns_to_keep = self.config.required_columns + [factor]\n",
    "            factor_df = df[columns_to_keep].copy()\n",
    "            factor_dfs[factor] = factor_df\n",
    "            \n",
    "        return factor_dfs\n",
    "\n",
    "    def save_factor_df(self, factor_df: pd.DataFrame, base_name: str, factor: str, has_nan: bool = False, has_missing_dates: bool = False) -> None:\n",
    "        \"\"\"Save individual factor DataFrame with appropriate filename\"\"\"\n",
    "        \n",
    "        factor_name = factor.replace('/', '_').replace('\\\\', '_').replace(':', '_').replace('*', '_') \\\n",
    "                            .replace('?', '_').replace('\"', '_').replace('<', '_').replace('>', '_').replace('|', '_')\n",
    "        \n",
    "        # Construct filename with factor name first\n",
    "        filename = f\"{base_name} ({factor_name})\"\n",
    "        \n",
    "        # Add suffixes in the desired order\n",
    "        if has_nan:\n",
    "            filename = f\"{filename} -fillna\"\n",
    "            \n",
    "        if has_missing_dates:\n",
    "            filename = f\"{filename} -md\"\n",
    "        \n",
    "        # Add the extension\n",
    "        filename = f\"{filename}.csv\"\n",
    "        \n",
    "        factor_df.to_csv(filename, index=False)     \n",
    "\n",
    "    def process_single_file(self, file_path: str, dest_folder: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Process a single CSV file and return log messages\"\"\"\n",
    "        logs = {\n",
    "            'empty_data': [],\n",
    "            'missing_dates': [],\n",
    "            'nan': [],\n",
    "            'duplicates': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            filename = os.path.basename(file_path)\n",
    "            df = pd.read_csv(file_path)\n",
    "            base_output_path = os.path.join(dest_folder, os.path.splitext(filename)[0])\n",
    "            \n",
    "            # Check dates\n",
    "            is_continuous, missing_dates = self.check_dates(df)\n",
    "            has_missing_dates = False\n",
    "            \n",
    "            if isinstance(missing_dates, str):\n",
    "                logs['empty_data'].append(f\"{filename} has an empty or invalid date column.\")\n",
    "                return logs\n",
    "            \n",
    "            if not is_continuous:\n",
    "                has_missing_dates = True\n",
    "                min_date = df[self.config.time_column].min()\n",
    "                max_date = df[self.config.time_column].max()\n",
    "                total_dates = pd.date_range(start=min_date, end=max_date, freq=self.config.frequency)\n",
    "                missing_percentage = (len(missing_dates) / len(total_dates)) * 100\n",
    "                \n",
    "                logs['missing_dates'].extend([\n",
    "                    f\"{filename} has missing dates: {', '.join(missing_dates.astype(str))}\",\n",
    "                    f\"Missing {len(missing_dates)} out of {len(total_dates)} ({missing_percentage:.2f}%)\\n\"\n",
    "                ])\n",
    "\n",
    "            # Rename volume columns if needed\n",
    "            if 'volume_x' in df.columns:\n",
    "                df = df.rename(columns={'volume_x': 'volume'})\n",
    "            \n",
    "            # Drop unwanted columns\n",
    "            existing_columns = [col for col in self.config.columns_to_drop if col in df.columns]\n",
    "            if existing_columns:\n",
    "                df.drop(columns=existing_columns, inplace=True)\n",
    "            \n",
    "            # Handle duplicates\n",
    "            original_rows = len(df)\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            if len(df) < original_rows:\n",
    "                logs['duplicates'].append(\n",
    "                    f\"{filename} has duplicates and {original_rows - len(df)} row(s) were removed.\"\n",
    "                )\n",
    "            \n",
    "            # Separate factors\n",
    "            factor_dfs = self.separate_and_save_factors(df, base_output_path)\n",
    "            \n",
    "            # Handle NaN values\n",
    "            for factor, factor_df in factor_dfs.items():\n",
    "                has_nan = factor_df.isna().any().any()\n",
    "                if has_nan:\n",
    "                    factor_df.fillna(0, inplace=True)\n",
    "                    logs['nan'].append(f\"{filename} ({factor}) has NaN values and have filled with 0.\")\n",
    "                \n",
    "                # Save the processed factor DataFrame\n",
    "                self.save_factor_df(\n",
    "                    factor_df,\n",
    "                    base_output_path,\n",
    "                    factor,\n",
    "                    has_nan=has_nan,\n",
    "                    has_missing_dates=has_missing_dates\n",
    "                )\n",
    "            \n",
    "            return logs\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "            return logs\n",
    "\n",
    "    def parallel_process_files(self, folder_name: str) -> None:\n",
    "        \"\"\"Parallel process all csv files in a folder\"\"\"\n",
    "        try:\n",
    "            ori_folder = os.path.join(self.config.base_ori_folder, folder_name)\n",
    "            dest_folder = os.path.join(self.config.base_dest_folder, folder_name)\n",
    "            os.makedirs(dest_folder, exist_ok=True)\n",
    "            \n",
    "            self.logger.info(f\"Processing folder: {folder_name}\")\n",
    "            \n",
    "            # Process all CSV files in parallel\n",
    "            csv_files = [f for f in os.listdir(ori_folder) if f.endswith('.csv')]\n",
    "            all_logs = {\n",
    "                'empty_data': [],\n",
    "                'missing_dates': [],\n",
    "                'nan': [],\n",
    "                'duplicates': []\n",
    "            }\n",
    "            \n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = [\n",
    "                    executor.submit(\n",
    "                        self.process_single_file,\n",
    "                        os.path.join(ori_folder, file),\n",
    "                        dest_folder\n",
    "                    )\n",
    "                    for file in csv_files\n",
    "                ]\n",
    "                \n",
    "                for future in futures:\n",
    "                    logs = future.result()\n",
    "                    for key in logs:\n",
    "                        all_logs[key].extend(logs[key])\n",
    "            \n",
    "            # Write logs\n",
    "            for log_type, messages in all_logs.items():\n",
    "                self.write_log(\n",
    "                    self.get_log_file_path(dest_folder, log_type, folder_name),\n",
    "                    messages\n",
    "                )\n",
    "                \n",
    "            self.logger.info(f\"Completed processing {folder_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing folder {folder_name}: {str(e)}\")\n",
    "\n",
    "    def process_all_folders(self) -> None:\n",
    "        \"\"\"Process all folders in the base directory\"\"\"\n",
    "        all_folders = [\n",
    "            f for f in os.listdir(self.config.base_ori_folder)\n",
    "            if os.path.isdir(os.path.join(self.config.base_ori_folder, f))\n",
    "        ]\n",
    "        \n",
    "        for folder in all_folders:\n",
    "            self.parallel_process_files(folder)\n",
    "\n",
    "def main():\n",
    "    config = ProcessingConfig()\n",
    "    processor = DataProcessor(config)\n",
    "    processor.process_all_folders()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_ori_folder structure #\n",
    "\n",
    "# Ori Data\n",
    "# └── Folder 1\n",
    "#     └── csv\n",
    "# └── Folder 2\n",
    "#     └── csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybotrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
